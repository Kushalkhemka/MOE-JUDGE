"""
Convert human judged datasets from "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning" (Golovneva et al., ICLR 2023).
Only consider metrics that consider the overall answer generated by the model.

Original files located at:
    https://dl.fbaipublicfiles.com/parlai/projects/roscoe/annotations.zip
"""

import os
import pandas as pd
from typing import Any

from utils.utils import (
    read_csv,
    read_jsonl,
    parse_reasoning_chain,
    compare_strings,
    split_substeps,
    save_dict_to_json,
)


DATASET_NAMES = [
    "drop",
    "cosmos",
    "esnli",
    "gsm8k",
]

SCHEMA = {
    "dataset_url": "https://dl.fbaipublicfiles.com/parlai/projects/roscoe/annotations.zip",
    "annotations": [
        {
            "metric": "Overall Quality",
            "category": "graded",
            "prompt": "{{ instance }}Does the generated response answer the question in a well-justified manner? (1=incomprehensible and wrong, 5=clear and correct)",
            "worst": 1,
            "best": 5,
        },
        {
            "metric": "Coherency",
            "category": "graded",
            "prompt": "{{ instance }}Does the whole generated response make sense? (Ie, does it sound understandable/non-contradictory/sensible, even if it fails to address the context?) - (1=sounds like nonsense, 5=easy to parse).",
            "worst": 1,
            "best": 5,
        },
        {
            "metric": "Missing Steps",
            "category": "categorical",
            "prompt": "{{ instance }}Is the reasoning in the generated response incomplete and lacking required information to produce the correct answer? Specifically, does this response contain steps that, if added in, would make for a well-supported chain?",
            "labels_list": ["yes", "no"],
        },
        {
            "metric": "Contradiction",
            "category": "categorical",
            "prompt": "{{ instance }}Do steps contradict each other or fail to follow a cohesive story?",
            "labels_list": ["yes", "no"],
        },
    ],
    "expert_annotator": "true",
    "original_prompt": True,
}


def generate_prompt(
    premise: str, hypothesis: str, correct_relationship: str, generated_response: str
) -> str:
    """Generate prompt for datasets other than gsm8k.

    Args:
        premise (str): The premise of the problem statement.
        hypothesis (str): The hypothesis of the problem statement.
        correct_relationship (str): The correct relationship of the problem statement.
        generated_response (str): The model's generated response.

    Returns:
        str: The full prompt.
    """
    return (
        'For this task, you will be shown a CONTEXT with a "Situation" and a "Claim" about that "Situation". The "Claim" may or may not be supported by the "Situation". The Correct Relationship between the "Claim" and the "Situation" is provided.\n\n'
        "You will be shown a GENERATED RESPONSE generated from a bot, asked the question\n\nIs the Claim supported by the Situation?\n\nYou will be asked to JUDGE THE GENERATED RESPONSE as a whole. Interpret the questions to the best of your ability. "
        'Sometimes the generated response will refer to the "Situation" as a "Premise" and the "Claim" as a "Hypothesis". It will oftentimes be faster to read the "Claim" before the "Situation".\n\n'
        f"CONTEXT:\nSituation (Premise): {premise}\n\nClaim (Hypothesis): {hypothesis}\n\nIs the Claim supported by the Situation?\n\nCorrect Relationship (Yes or No): {correct_relationship}\n\nGENERATED RESPONSE:\n{generated_response}\n"
    )


def generate_gsm8k_prompt(
    question: str, correct_answer: str, generated_response: str
) -> str:
    """Generate gsm8k-specific prompt.

    Args:
        question (str): The problem statement.
        correct_answer (str): The ground truth solution to the problem statement.
        generated_response (str): The model's generated response.

    Returns:
        str: The full prompt.
    """
    return (
        'For this task, you will be shown a CONTEXT with a "Question" and a corresponding "Solution".\n\n'
        'You will be shown a GENERATED RESPONSE generated from a bot, asked to solve the "Question".\n\nYou will be asked to JUDGE THE GENERATED RESPONSE as a whole. Interpret the questions to the best of your ability.\n\n'
        f"CONTEXT:\nQuestion: {question}\n\Solution: {correct_answer}\n\nGENERATED RESPONSE:\n{generated_response}\n"
    )


def create_instance(
    id: int, context_prompt: str, annotation_data: pd.Series
) -> dict[str, Any]:
    """
    A function to create an instance with annotations for overall quality, coherency, missing steps, and contradiction based on the provided parameters.

    Args:
        id (int): The ID of the instance.
        context_prompt (str): The context prompt for the instance.
        annotation_data (pd.Series): The DataSeries containing annotation data.

    Returns:
        dict[str, Any]: A dictionary representing the instance with annotations.
    """
    instruction = "Judge the generated response:\n"

    # assemble annotations
    overall_quality = annotation_data["0_full_newOverall_result"]
    coherency = annotation_data["0_full_newCoherent_result"]
    missing_steps = annotation_data["0_full_newMissingStep_result"]
    contradiction = annotation_data["0_full_newClearContradiction_result"]

    annotations = {
        "Overall Quality": {
            "mean_human": overall_quality,
            "majority_human": overall_quality,
            "individual_human_scores": [overall_quality],
        },
        "Coherency": {
            "mean_human": coherency,
            "majority_human": coherency,
            "individual_human_scores": [coherency],
        },
        "Missing Steps": {
            "majority_human": missing_steps,
            "individual_human_scores": [missing_steps],
        },
        "Contradiction": {
            "majority_human": contradiction,
            "individual_human_scores": [contradiction],
        },
    }

    instance = {
        "id": id,
        "instance": context_prompt + instruction,
        "annotations": annotations,
    }

    return instance


def create_input_prompt(
    dataset_name: str, context: dict[str, str], generated_response: str
) -> str:
    """
    Generates an input prompt based on the dataset name.

    Args:
        dataset_name (str): The name of the dataset.
        context (dict[str, str]): A dictionary containing context information.
        generated_response (str): The model's generated response.

    Returns:
        str: The input prompt generated based on the dataset name.
    """
    if dataset_name == "gsm8k":
        return generate_gsm8k_prompt(
            question=context["premise"].strip(),
            correct_answer=context["hypothesis"].split(
                "IGNORE THIS. Ground truth here for reference. "
            )[-1],
            generated_response=generated_response,
        )
    else:
        return generate_prompt(
            premise=context["premise"].strip(),
            hypothesis=context["hypothesis"].strip(),
            correct_relationship=context["answer"].strip(),
            generated_response=generated_response,
        )


def assemble_instances(
    context_file: str, annotation_file: str, dataset_name: str
) -> list[dict]:
    """
    Assembles instances of the dataset.

    Args:
        context_file (str): The file containing context data.
        annotation_file (str): The file containing annotation data.
        dataset_name (str): The name of the dataset.

    Returns:
        list[dict]: A list of dictionaries representing the assembled instances.
    """
    instances_overall: list[dict] = []

    # read data
    context_data = read_jsonl(context_file)
    annotation_data = read_csv(annotation_file)

    for idx, annotation_row in annotation_data.iterrows():
        meta_data_idx = annotation_row["metadata_example_idx"]
        context = context_data[meta_data_idx]

        # reasoning chain
        reasoning_chain_context = context["gpt-3"].strip()
        reasoning_chain_annotations = parse_reasoning_chain(
            annotation_row["metadata_generation"]
        ).strip()

        assert compare_strings(
            reasoning_chain_context, reasoning_chain_annotations
        ), f"Annotation reasoning chain for idx {idx} does not match with context!\n{reasoning_chain_context}\n\n{reasoning_chain_annotations}"

        # assemble instance
        reasoning_steps = split_substeps(annotation_row["metadata_generation"])
        generated_response = "\n".join(reasoning_steps)

        context_prompt = create_input_prompt(
            dataset_name=dataset_name,
            context=context,
            generated_response=generated_response,
        )

        # get overall instance
        overall_instance = create_instance(
            id=idx + 1, context_prompt=context_prompt, annotation_data=annotation_row
        )
        instances_overall.append(overall_instance)

    return instances_overall


if __name__ == "__main__":
    context_data_path = os.path.join("original_data", "context")
    annotation_data_path = os.path.join("original_data", "annotated")

    for dataset_name in DATASET_NAMES:
        converted_dataset_name = f"roscoe-{dataset_name}-overall"
        context_file = os.path.join(context_data_path, f"{dataset_name}.jsonl")
        annotation_file = os.path.join(annotation_data_path, f"{dataset_name}.csv")

        instances = assemble_instances(context_file, annotation_file, dataset_name)

        # assemble dataset
        dataset_schema: dict[str, Any] = {
            "dataset": f"{converted_dataset_name} (Golovneva et al., ICLR 2023)"
        }
        dataset_schema.update(SCHEMA)
        dataset_schema["instances"] = instances

        # save to dics
        save_dict_to_json(dataset_schema, f"{converted_dataset_name}.json")
